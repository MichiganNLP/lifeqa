```bibtex
@InProceedings{castro-EtAl:2020:LREC,
  author    = {Castro, Santiago  and  Azab, Mahmoud  and  Stroud, Jonathan  and  Noujaim, Cristina  and  Wang, Ruoyao
      and  Deng, Jia  and  Mihalcea, Rada},
  title     = {LifeQA: A Real-life Dataset for Video Question Answering},
  booktitle = {Proceedings of The 12th Language Resources and Evaluation Conference},
  month     = {May},
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  pages     = {4352--4358},
  abstract  = {We introduce LifeQA, a benchmark dataset for video question answering that focuses on day-to-day real-life situations. Current video question answering datasets consist of movies and TV shows. However, it is well-known that these visual domains are not representative of our day-to-day lives. Movies and TV shows, for example, benefit from professional camera movements, clean editing, crisp audio recordings, and scripted dialog between professional actors. While these domains provide a large amount of data for training models, their properties make them unsuitable for testing real-life question answering systems. Our dataset, by contrast, consists of video clips that represent only real-life scenarios. We collect 275 such video clips and over 2.3k multiple-choice questions. In this paper, we analyze the challenging but realistic aspects of LifeQA, and we apply several state-of-the-art video question answering models to provide benchmarks for future research. The full dataset is publicly available at https://lit.eecs.umich.edu/lifeqa/.},
  url       = {https://www.aclweb.org/anthology/2020.lrec-1.536}
}
```
